{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBJpzAUA060I"
   },
   "source": [
    "# Homework 4\n",
    "\n",
    "Gabriel Idris Gilling (gig2106@columbia.edu) & Juan Lopez-Martin (jl5522@columbia.edu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxSSA4YUDuWE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso, ElasticNet, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit, RepeatedStratifiedKFold, GridSearchCV,train_test_split, cross_val_score\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Bf66X7oOj8E"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable = [\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "wSrANUEWFY9S",
    "outputId": "1cedf2b6-d5f8-49fc-dd3a-fa92fa3cffb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'country', 'description', 'designation', 'points',\n",
       "       'price', 'province', 'region_1', 'region_2', 'taster_name',\n",
       "       'taster_twitter_handle', 'title', 'variety', 'winery'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"drive/My Drive/ML/winemag-data-130k-v2.csv\")\n",
    "df = df.sample(10000, random_state= 2020)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeary6tpTCgu"
   },
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ThT95oU2_mu"
   },
   "source": [
    "We start building a model based on some of the features except for the description. Using LASSO, we get a $R^2$ of 0.34."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCufLxBpFZCC"
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df[['country', 'province', 'region_1','region_2', 'taster_name', 'variety']], dummy_na=True)\n",
    "X['price'] = df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['points'], random_state = 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4Y79JYvHR-48",
    "outputId": "8b3677b0-a904-4a95-cbfc-9471d86c595f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34453290345428"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctransformer = make_column_transformer((make_pipeline(StandardScaler(), SimpleImputer()), X.columns=='price'), \n",
    "                                       remainder='passthrough')\n",
    "\n",
    "pipe_lasso = make_pipeline(ctransformer, Lasso())\n",
    "\n",
    "gridlasso = GridSearchCV(estimator=pipe_lasso, param_grid={'lasso__alpha': [1, 0.1, 0.01, 0.0001, 0.00001]}, cv=3)\n",
    "gridlasso.fit(X_train, y_train)\n",
    "gridlasso.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKzgUBaB2rkN"
   },
   "source": [
    "We get a similar score using Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gNT2PMw2LhS7",
    "outputId": "2b7f0a7a-5328-488f-cf9e-dd18269773ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37229963994365145"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rf = make_pipeline(ctransformer, RandomForestRegressor())\n",
    "\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "pipe_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uapxYCD2WT3e"
   },
   "source": [
    "### 1.2 Simple Model using Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zb1TxH9_5w3a"
   },
   "source": [
    "Using a very simple BoW model with CountVectorizer gets us a baseline of $R^2 = 0.6$. Note that we are using max_features = 2000 to reduce computing time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BwtukhXsWYnW",
    "outputId": "ca4ab7b8-b1b1-4681-d9ad-0700db900945"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6063213315131453"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=2000)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(df['description'], df['points'], random_state = 2020)\n",
    "\n",
    "X1_train_bow = vect.fit_transform(X1_train)\n",
    "X1_test_bow = vect.transform(X1_test)\n",
    "\n",
    "lr = LassoCV().fit(X1_train_bow, y1_train)\n",
    "lr.score(X1_test_bow, y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3XcDgAgT1d7"
   },
   "source": [
    "### 1.3 Simple Model with preprocessing, TF-IDF and bigrams\n",
    "\n",
    "To start, we define a simple preprocessing function that lemmatizes our Wine descriptions after keeping only words that are constituted of characters and are not stopwords. Previous trials have shown that this basic preprocessing increases our models' accuracy by a small but significant margin.\n",
    "\n",
    "We then deploy the TF-IDF vectorizer, setting the ngram_range argument to (1,2), instructing it to find bigrams and transform the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zccac_qr-ixG"
   },
   "outputs": [],
   "source": [
    "def preprocess2(series):\n",
    "    series = series.apply(lambda x : [w.lemma_ for w in nlp(x) if w.is_alpha and not w.is_stop])\n",
    "\n",
    "    return (series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "MjPjVd7iTc4Y",
    "outputId": "a0bf2147-196a-4bc6-c1b8-49e79b98087c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91775    [barrel, selection, age, quarter, new, oak, no...\n",
       "33035    [attractive, fresh, Chablis, flavor, juicy, cu...\n",
       "85640    [blend, Pinot, Noir, Lemberger, Lemberger, dom...\n",
       "54044    [Soft, bright, red, berry, fruit, warm, open, ...\n",
       "43602    [savory, wine, open, aroma, cure, meat, cedar,...\n",
       "Name: description_preprocess, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description_preprocess'] = preprocess2(df['description'])\n",
    "df['description_preprocess'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FGIAJcS_3X1"
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df[['country', 'province', 'region_1','region_2', 'taster_name', 'variety']], dummy_na=True)\n",
    "X['price'] = df['price']\n",
    "X['description_preprocess'] = df['description_preprocess']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['points'], random_state = 2020)\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), max_features=2000)\n",
    "X_train_tfidf = vect.fit_transform(X_train['description_preprocess'].apply(lambda x: ' '.join(x)))\n",
    "X_test_tfidf = vect.transform(X_test['description_preprocess'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-kMwKL-eDNhI",
    "outputId": "9823ecc5-a12a-42f8-fb72-2a0af1935346"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5581082057859612"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LassoCV().fit(X_train_tfidf, y_train)\n",
    "lr.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DLIrtoBMHu3"
   },
   "source": [
    "Surprisingly, we get a slightly worse score than the one using CountVectorizer. We also removed the use of bigrams to check if that could improve the score, but the increment was minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MlUw-Mq4KvyX",
    "outputId": "128032e8-d2ed-4c60-bdfe-d542a2a015e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5669091466129174"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(max_features=2000)\n",
    "X_train_tfidf = vect.fit_transform(X_train['description_preprocess'].apply(lambda x: ' '.join(x)))\n",
    "X_test_tfidf = vect.transform(X_test['description_preprocess'].apply(lambda x: ' '.join(x)))\n",
    "lr = LassoCV().fit(X_train_tfidf, y_train)\n",
    "lr.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qErSuUhcX86U"
   },
   "source": [
    "## 1.4 Merging BoW with non-text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSwRVlG7WMK2"
   },
   "source": [
    "Although there is a small improvement when including the non-textual features, we expected a greater increase. Potentially, a better model could increase this score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9As3A7SUB-H"
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df[['country', 'province', 'region_1','region_2', 'taster_name', 'variety']], dummy_na=True)\n",
    "X['price'] = df['price']\n",
    "X['description'] = df['description']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['points'], random_state = 2020)\n",
    "\n",
    "vect = CountVectorizer(max_features=2000)\n",
    "X_train_bow = vect.fit_transform(X_train['description'])\n",
    "X_test_bow = vect.transform(X_test['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jG8I62hRNfJE"
   },
   "outputs": [],
   "source": [
    "X_train_all = np.concatenate((X_train_bow.todense(), X_train.loc[:, X_train.columns != 'description'].to_numpy()), axis = 1)\n",
    "X_test_all = np.concatenate((X_test_bow.todense(), X_test.loc[:, X_train.columns != 'description'].to_numpy()), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TGrtZaIhNfTv",
    "outputId": "f63b4b0b-8b8c-4d97-ece1-7675e5f903d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6215606800093575"
      ]
     },
     "execution_count": 103,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition = np.concatenate(([False]*(X_train_all.shape[1]-1), [True]), axis = 0)\n",
    "ctransformer = make_column_transformer((make_pipeline(StandardScaler(), SimpleImputer()), condition), \n",
    "                                       remainder='passthrough')\n",
    "\n",
    "pipe_lasso = make_pipeline(ctransformer, Lasso())\n",
    "\n",
    "gridlasso = GridSearchCV(estimator=pipe_lasso, param_grid={'lasso__alpha': [0.01, 0.001, 0.0001, 0.00001]}, cv=3)\n",
    "gridlasso.fit(X_train_all, y_train)\n",
    "gridlasso.score(X_test_all, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cf9F9UeuUVN2"
   },
   "source": [
    "# Ex 2\n",
    "\n",
    "In the section below, we use Spacy's implementation of Word2Vec to predict the wine qualities.\n",
    "\n",
    "First, we use a standalone model using Word2vec only.\n",
    "\n",
    "Afterwards, we merge out TF-IDF model with our Word2Vec models, and we get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o24Tr0R9Ms8l"
   },
   "source": [
    "### a) Standalone Spacy Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqH8BnqOPI7b"
   },
   "outputs": [],
   "source": [
    "X3_train, X3_test, y3_train, y3_test = train_test_split(df['description_preprocess'].apply(lambda x: ' '.join(x)), df['points'], random_state = 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "QuLGjSwKPSqD",
    "outputId": "5a211e1c-babb-4f1e-a9ff-0f1e240d0e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32773     light fruity wine already attractively fresh w...\n",
       "42403     flying pig blend cabernet franc merlot burstin...\n",
       "107224    old vines aegerter domaine give rounded ripe w...\n",
       "55725     hard mountain tannins characterize cab grown f...\n",
       "109593    bright raspberry blackberry flavors full body ...\n",
       "                                ...                        \n",
       "74472     wine bold full bodied big enough grilled steak...\n",
       "110260    grauburgunder aka pinot gris warm vineyards gi...\n",
       "109494    blackberry cherry notes lush ripe nose finger ...\n",
       "87440     strong dusty stringy herbal notes run right wi...\n",
       "24790     wine shows pedigree style red berry fruit flav...\n",
       "Name: description_preprocess, Length: 7500, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9Etof6dOFJY"
   },
   "outputs": [],
   "source": [
    "X_docs_train = [nlp(d).vector for d in X3_train]\n",
    "X_docs_test = [nlp(d).vector for d in X3_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Ws44o16P8fP"
   },
   "outputs": [],
   "source": [
    "X_docs_train = np.vstack(X_docs_train)\n",
    "X_docs_test = np.vstack(X_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rGuu6G7JP8io",
    "outputId": "c7439b26-b4a7-45cb-a9ec-a47e75d52eb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 300)"
      ]
     },
     "execution_count": 103,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_docs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kY1mhjorP8la"
   },
   "outputs": [],
   "source": [
    "lr = LassoCV().fit(X_docs_train, y3_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTlp2pj_snrC"
   },
   "source": [
    "The standalone model using only text features that were converted to Word2Vec format using spacy achieves an $R^2$ of nearly 0.5, which is relatively low compared to our previous models, but understandable given the fact that we're only using a single column in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PeiZMV12P8or",
    "outputId": "dc703201-07cd-408c-bf70-33075fbb0771"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4946833690189919"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_docs_test, y3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9RKIpFzRY8_"
   },
   "source": [
    "## b) Let's stack Word2vec and BoW\n",
    "\n",
    "We want to see whether we can merge our vectorized representations of text features in both TF-IDF format to achieve a better result.\n",
    "\n",
    "First, we run a simple Lasso model using the TFIDF vectorizer on our descriptions to establish a baseline. We can see that the standalone TFIDF model, using 2000 features performs better than the Spacy word2vec embeddings. We are keeping 2000 features to speed up computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-LcQDvMURdZ0",
    "outputId": "a84813fe-15e9-4e4b-b63f-d6f783e88b95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5695884516324916"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(stop_words=None, max_features= 2000)\n",
    "X3_train_tfidf = vect.fit_transform(X3_train)\n",
    "X3_test_tfidf = vect.transform(X3_test)\n",
    "\n",
    "lr = LassoCV().fit(X3_train_tfidf, y3_train)\n",
    "lr.score(X3_test_tfidf, y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zky89L-NZfQ"
   },
   "outputs": [],
   "source": [
    "X_docs_train = [nlp(d).vector for d in X3_train]\n",
    "X_docs_test = [nlp(d).vector for d in X3_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_i5p6Ru6tvLB"
   },
   "source": [
    "In order to concatenate our TF-IDF vectorized representations with the Spacy word embeddings, we first need to transform them into dense format, while also converting the word embeddings back to Numpy array format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgOkTn0OPcs8"
   },
   "outputs": [],
   "source": [
    "X3_train_comb = np.concatenate((X3_train_tfidf.todense(), np.array(X_docs_train)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIcFcLQYQv-S"
   },
   "outputs": [],
   "source": [
    "X3_test_comb = np.concatenate((X3_test_tfidf.todense(), np.array(X_docs_test)), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lwe3CzBauQGL"
   },
   "source": [
    "We achieve an $R^2$ of 0.61 which is higher than our baseline for both the standalone word2vec embeddings and TFIDF models, effectively demonstrating that combining both types of features can increase model accuracy by a substantive margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5AS6BU8JNyLY",
    "outputId": "1cc9ee55-b75f-4b34-a096-e8af7cb265e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6109122006080508"
      ]
     },
     "execution_count": 137,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LassoCV().fit(X3_train_comb, y3_train)\n",
    "lr.score(X3_test_comb, y3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qucf2jZwyr9c"
   },
   "source": [
    "# Part 3\n",
    "Our code is based on McCormick and Ryan's tutorial [BERT Fine-Tuning Tutorial with PyTorch](https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=EKOTlwcmxmej) which is in turn based on the [run_glue.py](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py) script in the transformers library.\n",
    "\n",
    "However, there are important differences in our implementation. \n",
    "* First and foremost, instead of a classification problem we are trying to solve a regression problem. Therefore, the last layer is of the model only has one neuron with a linear activation function that thus returns a continous numerical value. We use MSE as the loss and R^2 as the metric to easily compare with the previous models.\n",
    "* Second, we are using [ALBERT](https://arxiv.org/abs/1909.11942) instead of BERT. This should make training faster while keeping a similar performance.\n",
    "* We use a batch size of 32 and 4 epochs instead of 2. Some of the hyperparameters are slightly modified to follow the recommendations for ALBERT, still using AdamW.\n",
    "\n",
    "For now we are using a random sample of 100,000 rows instead of the full dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vz7WFG3mz4sm"
   },
   "outputs": [],
   "source": [
    "seed_val = 2020\n",
    "\n",
    "sample = 100000\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "OiQS-y7jjwAc",
    "outputId": "7d483519-6567-4f9e-c1b3-53f56de1d1cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from transformers import AlbertForSequenceClassification, AdamW, AlbertConfig\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "df = pd.read_csv(\"drive/My Drive/ML/winemag-data-130k-v2.csv\")\n",
    "df = df.sample(sample)\n",
    "df.columns\n",
    "\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BqI89F-rNV-q",
    "outputId": "f112eae8-455e-4772-d28b-a4eed3f50476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80,000 training samples\n",
      "20,000 validation samples\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  2,500.    Elapsed: 0:00:36.\n",
      "  Batch   100  of  2,500.    Elapsed: 0:01:13.\n",
      "  Batch   150  of  2,500.    Elapsed: 0:01:49.\n",
      "  Batch   200  of  2,500.    Elapsed: 0:02:26.\n",
      "  Batch   250  of  2,500.    Elapsed: 0:03:02.\n",
      "  Batch   300  of  2,500.    Elapsed: 0:03:38.\n",
      "  Batch   350  of  2,500.    Elapsed: 0:04:15.\n",
      "  Batch   400  of  2,500.    Elapsed: 0:04:51.\n",
      "  Batch   450  of  2,500.    Elapsed: 0:05:27.\n",
      "  Batch   500  of  2,500.    Elapsed: 0:06:04.\n",
      "  Batch   550  of  2,500.    Elapsed: 0:06:40.\n",
      "  Batch   600  of  2,500.    Elapsed: 0:07:17.\n",
      "  Batch   650  of  2,500.    Elapsed: 0:07:53.\n",
      "  Batch   700  of  2,500.    Elapsed: 0:08:29.\n",
      "  Batch   750  of  2,500.    Elapsed: 0:09:06.\n",
      "  Batch   800  of  2,500.    Elapsed: 0:09:42.\n",
      "  Batch   850  of  2,500.    Elapsed: 0:10:19.\n",
      "  Batch   900  of  2,500.    Elapsed: 0:10:55.\n",
      "  Batch   950  of  2,500.    Elapsed: 0:11:31.\n",
      "  Batch 1,000  of  2,500.    Elapsed: 0:12:08.\n",
      "  Batch 1,050  of  2,500.    Elapsed: 0:12:44.\n",
      "  Batch 1,100  of  2,500.    Elapsed: 0:13:21.\n",
      "  Batch 1,150  of  2,500.    Elapsed: 0:13:57.\n",
      "  Batch 1,200  of  2,500.    Elapsed: 0:14:33.\n",
      "  Batch 1,250  of  2,500.    Elapsed: 0:15:10.\n",
      "  Batch 1,300  of  2,500.    Elapsed: 0:15:46.\n",
      "  Batch 1,350  of  2,500.    Elapsed: 0:16:23.\n",
      "  Batch 1,400  of  2,500.    Elapsed: 0:16:59.\n",
      "  Batch 1,450  of  2,500.    Elapsed: 0:17:35.\n",
      "  Batch 1,500  of  2,500.    Elapsed: 0:18:12.\n",
      "  Batch 1,550  of  2,500.    Elapsed: 0:18:48.\n",
      "  Batch 1,600  of  2,500.    Elapsed: 0:19:25.\n",
      "  Batch 1,650  of  2,500.    Elapsed: 0:20:01.\n",
      "  Batch 1,700  of  2,500.    Elapsed: 0:20:37.\n",
      "  Batch 1,750  of  2,500.    Elapsed: 0:21:14.\n",
      "  Batch 1,800  of  2,500.    Elapsed: 0:21:50.\n",
      "  Batch 1,850  of  2,500.    Elapsed: 0:22:27.\n",
      "  Batch 1,900  of  2,500.    Elapsed: 0:23:03.\n",
      "  Batch 1,950  of  2,500.    Elapsed: 0:23:39.\n",
      "  Batch 2,000  of  2,500.    Elapsed: 0:24:16.\n",
      "  Batch 2,050  of  2,500.    Elapsed: 0:24:52.\n",
      "  Batch 2,100  of  2,500.    Elapsed: 0:25:29.\n",
      "  Batch 2,150  of  2,500.    Elapsed: 0:26:05.\n",
      "  Batch 2,200  of  2,500.    Elapsed: 0:26:41.\n",
      "  Batch 2,250  of  2,500.    Elapsed: 0:27:18.\n",
      "  Batch 2,300  of  2,500.    Elapsed: 0:27:54.\n",
      "  Batch 2,350  of  2,500.    Elapsed: 0:28:31.\n",
      "  Batch 2,400  of  2,500.    Elapsed: 0:29:07.\n",
      "  Batch 2,450  of  2,500.    Elapsed: 0:29:43.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:30:20\n",
      "\n",
      "Running Validation...\n",
      "  R2: 0.72\n",
      "  Validation Loss: 0.01\n",
      "  Validation took: 0:02:33\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  2,500.    Elapsed: 0:00:36.\n",
      "  Batch   100  of  2,500.    Elapsed: 0:01:13.\n",
      "  Batch   150  of  2,500.    Elapsed: 0:01:49.\n",
      "  Batch   200  of  2,500.    Elapsed: 0:02:26.\n",
      "  Batch   250  of  2,500.    Elapsed: 0:03:02.\n",
      "  Batch   300  of  2,500.    Elapsed: 0:03:38.\n",
      "  Batch   350  of  2,500.    Elapsed: 0:04:15.\n",
      "  Batch   400  of  2,500.    Elapsed: 0:04:51.\n",
      "  Batch   450  of  2,500.    Elapsed: 0:05:28.\n",
      "  Batch   500  of  2,500.    Elapsed: 0:06:04.\n",
      "  Batch   550  of  2,500.    Elapsed: 0:06:40.\n",
      "  Batch   600  of  2,500.    Elapsed: 0:07:17.\n",
      "  Batch   650  of  2,500.    Elapsed: 0:07:53.\n",
      "  Batch   700  of  2,500.    Elapsed: 0:08:29.\n",
      "  Batch   750  of  2,500.    Elapsed: 0:09:06.\n",
      "  Batch   800  of  2,500.    Elapsed: 0:09:42.\n",
      "  Batch   850  of  2,500.    Elapsed: 0:10:19.\n",
      "  Batch   900  of  2,500.    Elapsed: 0:10:55.\n",
      "  Batch   950  of  2,500.    Elapsed: 0:11:31.\n",
      "  Batch 1,000  of  2,500.    Elapsed: 0:12:08.\n",
      "  Batch 1,050  of  2,500.    Elapsed: 0:12:44.\n",
      "  Batch 1,100  of  2,500.    Elapsed: 0:13:21.\n",
      "  Batch 1,150  of  2,500.    Elapsed: 0:13:57.\n",
      "  Batch 1,200  of  2,500.    Elapsed: 0:14:33.\n",
      "  Batch 1,250  of  2,500.    Elapsed: 0:15:10.\n",
      "  Batch 1,300  of  2,500.    Elapsed: 0:15:46.\n",
      "  Batch 1,350  of  2,500.    Elapsed: 0:16:23.\n",
      "  Batch 1,400  of  2,500.    Elapsed: 0:16:59.\n",
      "  Batch 1,450  of  2,500.    Elapsed: 0:17:35.\n",
      "  Batch 1,500  of  2,500.    Elapsed: 0:18:12.\n",
      "  Batch 1,550  of  2,500.    Elapsed: 0:18:48.\n",
      "  Batch 1,600  of  2,500.    Elapsed: 0:19:25.\n",
      "  Batch 1,650  of  2,500.    Elapsed: 0:20:01.\n",
      "  Batch 1,700  of  2,500.    Elapsed: 0:20:37.\n",
      "  Batch 1,750  of  2,500.    Elapsed: 0:21:14.\n",
      "  Batch 1,800  of  2,500.    Elapsed: 0:21:50.\n",
      "  Batch 1,850  of  2,500.    Elapsed: 0:22:26.\n",
      "  Batch 1,900  of  2,500.    Elapsed: 0:23:03.\n",
      "  Batch 1,950  of  2,500.    Elapsed: 0:23:39.\n",
      "  Batch 2,000  of  2,500.    Elapsed: 0:24:16.\n",
      "  Batch 2,050  of  2,500.    Elapsed: 0:24:52.\n",
      "  Batch 2,100  of  2,500.    Elapsed: 0:25:29.\n",
      "  Batch 2,150  of  2,500.    Elapsed: 0:26:05.\n",
      "  Batch 2,200  of  2,500.    Elapsed: 0:26:41.\n",
      "  Batch 2,250  of  2,500.    Elapsed: 0:27:18.\n",
      "  Batch 2,300  of  2,500.    Elapsed: 0:27:54.\n",
      "  Batch 2,350  of  2,500.    Elapsed: 0:28:31.\n",
      "  Batch 2,400  of  2,500.    Elapsed: 0:29:07.\n",
      "  Batch 2,450  of  2,500.    Elapsed: 0:29:43.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:30:20\n",
      "\n",
      "Running Validation...\n",
      "  R2: 0.74\n",
      "  Validation Loss: 0.01\n",
      "  Validation took: 0:02:33\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:05:46 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "X = df['description'].to_numpy()\n",
    "y = torch.tensor((df['points'].to_numpy()-80)/20).float()\n",
    "nlabels = 1\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "def tokenize(sentences):\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  for sent in sentences:\n",
    "      encoded_dict = tokenizer.encode_plus(\n",
    "                          sent,                      # Sentence to encode.\n",
    "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                          max_length = 256,           # Pad & truncate all sentences.\n",
    "                          pad_to_max_length = True,\n",
    "                          return_attention_mask = True,   # Construct attn. masks.\n",
    "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )  \n",
    "      input_ids.append(encoded_dict['input_ids'])\n",
    "      attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "  return input_ids, attention_masks\n",
    "\n",
    "X_input, X_attention = tokenize(X)\n",
    "\n",
    "model = AlbertForSequenceClassification.from_pretrained(\n",
    "    \"albert-base-v2\",\n",
    "    num_labels = nlabels,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "model.cuda()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "\n",
    "dataset = TensorDataset(X_input, X_attention, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size)\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size)\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler as in run_glue.py\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss from McCormick and Ryan's tutorial\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset loss\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        loss = criterion(output[0].squeeze(), b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Average loss\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "      \n",
    "        with torch.no_grad():\n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            loss = criterion(output[0].squeeze(), b_labels)\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        output = output[0].squeeze().detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += r2_score(label_ids, output)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  R2: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWzc7KAp45h1"
   },
   "source": [
    "We get a final R^2 of 0.74. This is  better than any previous models we have trained, although note that all the others were trained on a smaller subset of the data. Note this score only takes into account the description; including other of the relevant features (e.g. price, denomination, etc.) would probably increase this number much more.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Hw4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
